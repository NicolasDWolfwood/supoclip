# The model size of OpenAI Whisper (see https://github.com/openai/whisper#available-models-and-languages)
WHISPER_MODEL_SIZE=medium
WHISPER_DEVICE=auto

# Transcription provider:
# - local (default): run Whisper locally
# - assemblyai: use AssemblyAI API
TRANSCRIPTION_PROVIDER=local

# LLM Providers
OPENAI_API_KEY=
GOOGLE_API_KEY=
ANTHROPIC_API_KEY=
ADMIN_API_KEY=

# The AI Model to use throughout the app
LLM=openai:gpt-5-mini

# Good for speed optimizations
COMPOSE_BAKE=true

# DB
POSTGRES_DB=supoclip
POSTGRES_USER=supoclip
POSTGRES_PASSWORD=supoclip_password

ASSEMBLY_AI_API_KEY=

# Worker concurrency (local transcription is CPU-heavy)
WORKER_MAX_JOBS=2

# Queue names for transcription routing
ARQ_QUEUE_NAME_LOCAL=arq:queue:local
ARQ_QUEUE_NAME_ASSEMBLY=arq:queue:assembly

# Optional second worker (compose profile: multi-worker)
WORKER2_MAX_JOBS=1
WORKER2_WHISPER_DEVICE=auto

# Host bind-mount location for Whisper model cache (docker-compose)
WHISPER_CACHE_HOST_DIR=./backend/.cache/whisper

# Optional GPU request override for second worker in compose profile
DOCKER_GPU_REQUEST_WORKER2=all

# Optional GPU request override for dedicated AssemblyAI worker
DOCKER_GPU_REQUEST_WORKER_ASSEMBLY=all

# Optional encryption secret for user-stored API keys
SECRET_ENCRYPTION_KEY=
