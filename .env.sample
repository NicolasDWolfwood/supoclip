# MrglSnips Environment Configuration
# Copy this file and fill in your API keys
# Canonical variable reference: docs/config.md

# ==============================================
# Local Host Mapping (single place for localhost URLs/ports)
# ==============================================

# Hostname used for browser-facing local URLs.
APP_HOST=localhost

# Host ports exposed by docker-compose.
FRONTEND_HOST_PORT=3000
BACKEND_HOST_PORT=8000
POSTGRES_HOST_PORT=5432
REDIS_HOST_PORT=6379

# Browser-facing URLs are derived from:
# - http://${APP_HOST}:${FRONTEND_HOST_PORT}
# - http://${APP_HOST}:${BACKEND_HOST_PORT}

# Optional comma-separated allowlist for browser origins that can call Better Auth endpoints.
# Keep localhost + 127.0.0.1 if you switch between both.
BETTER_AUTH_TRUSTED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# ==============================================
# AI Provider API Keys (Required for video processing)
# ==============================================

# AssemblyAI - Optional (only needed when TRANSCRIPTION_PROVIDER=assemblyai)
ASSEMBLY_AI_API_KEY=

# Choose one AI provider for transcript analysis:
# OpenAI (recommended: gpt-4.1, gpt-5)
OPENAI_API_KEY=

# Google AI (gemini-2.5-pro)
GOOGLE_API_KEY=

# Anthropic (claude-3-opus, claude-3-sonnet, claude-3-haiku)
ANTHROPIC_API_KEY=

# z.ai (GLM models via Coding API endpoint: https://api.z.ai/api/coding/paas/v4)
# Optional: you can also store per-user z.ai profile keys in Settings (subscription/metered)
# and choose routing mode (auto/subscription/metered). This env key remains a global fallback.
ZAI_API_KEY=

# ==============================================
# AI Model Configuration
# ==============================================

# Format: "provider:model-name"
# Examples:
#   - openai:gpt-4.1
#   - openai:gpt-5
#   - anthropic:claude-4-sonnet
#   - google:gemini-2.5-pro
#   - zai:glm-5
LLM=openai:gpt-5-mini

# Whisper model size (tiny, base, small, medium, large)
# Larger models are more accurate but slower
WHISPER_MODEL_SIZE=medium

# Whisper device:
# - auto (default): use CUDA when available, otherwise CPU
# - cuda: force GPU (falls back to CPU if unavailable)
# - cpu: force CPU
WHISPER_DEVICE=auto

# Transcription provider:
# - local (default): run Whisper locally, no remote transcript upload
# - assemblyai: use AssemblyAI API for transcription
TRANSCRIPTION_PROVIDER=local

# Worker concurrency (lower is safer for local Whisper on CPU)
WORKER_MAX_JOBS=2

# ARQ queue names (local + AssemblyAI)
ARQ_QUEUE_NAME_LOCAL=arq:queue:local
ARQ_QUEUE_NAME_ASSEMBLY=arq:queue:assembly

# Optional second worker (enabled with: docker compose --profile multi-worker up -d)
# Keep max jobs low per worker when using local Whisper.
WORKER2_MAX_JOBS=1
WORKER2_WHISPER_DEVICE=auto

# ==============================================
# Authentication
# ==============================================

# Better Auth secret (change this in production!)
# Generate a secure random string for production
BETTER_AUTH_SECRET=supoclip_dev_secret_change_in_production

# Optional admin key for destructive backend admin endpoints
# (e.g. POST /tasks/admin/cancel-all with x-admin-key header)
ADMIN_API_KEY=

# ==============================================
# Database Configuration
# ==============================================

# PostgreSQL credentials (matches docker-compose.yml)
POSTGRES_DB=supoclip
POSTGRES_USER=supoclip
POSTGRES_PASSWORD=supoclip_password

# ==============================================
# Docker Performance
# ==============================================

# Enable BuildKit for faster Docker builds
COMPOSE_DOCKER_CLI_BUILD=1
DOCKER_BUILDKIT=1

# Docker GPU request:
# - all (default): request all available GPUs for backend/worker
# - 0: disable GPU request and run on CPU
DOCKER_GPU_REQUEST=all

# Optional GPU request override for worker2 when profile multi-worker is enabled
# - all (default): request available GPUs
# - 0: force worker2 CPU-only execution
DOCKER_GPU_REQUEST_WORKER2=all

# GPU request for dedicated AssemblyAI worker
# - all (default): request available GPUs (required by compose GPU field)
DOCKER_GPU_REQUEST_WORKER_ASSEMBLY=all

# Optional encryption secret for user-stored API keys (recommended in production)
SECRET_ENCRYPTION_KEY=

# Optional host directory for Whisper model cache bind mount
# Avoids redownloading models after container rebuilds.
WHISPER_CACHE_HOST_DIR=./backend/.cache/whisper
